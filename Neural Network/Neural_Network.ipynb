{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer đầu tiên là input layer, các layer ở giữa được gọi là hidden layer, layer cuối cùng được gọi là\n",
    "output layer. Các hình tròn được gọi là node.\n",
    "\n",
    "Mỗi mô hình luôn có 1 input layer, 1 output layer, có thể có hoặc không các hidden layer. Tổng số\n",
    "layer trong mô hình được quy ước là số layer - 1 (không tính input layer)\n",
    "\n",
    "bias (hệ số tự do) la w0\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k, i: iterations\n",
    "Số node trong hidden layer thứ i là $l^{(i)}$\n",
    "\n",
    "Ma trận $W^{(k)}$ kích thước $l^{(k−1)} ∗ l^{(k)}$ là ma trận hệ số giữa layer (k-1) và layer k, trong đó $w^{(k)}_{ij}$\n",
    "là hệ số kết nối từ node thứ i của layer k-1 đến node thứ j của layer k.\n",
    "\n",
    "Vector $b^{(k)}$ kích thước $l^k ∗ 1$ là hệ số bias của các node trong layer k, trong đó $b^{(k)}_{i}$ là bias của node thứ i trong layer k.\n",
    "\n",
    "Với node thứ i trong layer l có bias $b^{(l)}_{i}$ thực hiện 2 bước:\n",
    "- Tính tổng linear: \n",
    ">$z^{(l)}_{i} = \\sum{l^{(l−1)}}{j=1}a^{(l−1)}_{j}∗w^{(l)}_{ji} +b^{(l)}_{i}$, là tổng tất cả các node trong layer trước nhân với hệ số w tương ứng, rồi cộng với bias b.\n",
    "- Áp dụng activation function: $a^{(l)}_i = σ(z^{(l)}_i)$\n",
    "\n",
    "Vector $z^{(k)}$ kích thước $l^{(k)}$ ∗ 1 là giá trị các node trong layer k sau bước tính tổng linear.\n",
    "\n",
    "Vector $a^{(k)}$ kích thước $l^{(k)}$ ∗ 1 là giá trị của các node trong layer k sau khi áp dụng hàm activation function.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
